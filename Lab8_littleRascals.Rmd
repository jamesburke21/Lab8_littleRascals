---
title: "lab8"
author: "Lil Rascals"
date: "3/6/2019"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(tidyverse)
```
###Team Question
How much has tuition changed over ten years. This is an important question, because seeing how data changes and how much tuition is increasing is an important issue for students. Also, it is essential to see whether tuition is increasing a reasonable amount or not. 
### Data/Expert
Here is a link to the data: https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-04-02
Here is where it originally came from: https://onlinembapage.com/average-tuition-and-educational-attainment-in-the-united-states/
Kelsey Mckenna is the pretend data expert. This question is important and interesting, because students shouldn't have to pay more than necessary for school. Tuition shouldn't increase more than inflation and if it does there is a problem with the system and action should be taken. Normal college aged students should be able to afford college and not have to pay absurb amounts of money in order to attend. 
###Kelsey Individual
Here is a link to the data: https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-04-02
```{r}

library(data.table)
library(tidyverse)

workout <- read_csv("week16_exercise.csv", sep = ";", 
                   header = T,
                   na.strings = c("NA", "","?"),
                   stringsAsFactors = FALSE)
workout <- workout[-c(53:364), ] %>% filter(exercise>27) %>% select(state, exercise) %>% arrange(desc(exercise))
workout
ggplot(data= workout)+
  geom_col(mapping = aes(x=state, y=exercise, fill = state))+
  coord_flip()

```
###Kelsey McKenna Findings
My question is which states were the top states in terms of exercise. 
I found that Colorado then Idaho are the top states in terms of exercise. 
The graph shows all states above 27 in the exercise column. 

### Shreeya Individual
```{r}
library(tidyverse)
library(DALEX2)

crime<-read_csv("crime_data_csv.csv", col_names = TRUE, skip = 3) 
crime_rates <-select(crime,Year, contains("rate")) %>%
  filter(!is.na(Year))%>%
  filter(row_number() <21) 
crime_rates[5,1]<- 2001 #fixed year name
crime_rates[19,1]<- 2015
tidy_crime_rates <- gather(crime_rates, contains("rate"), key = "type", value = "rate")

library(ggplot2)
crime_rates <- filter(tidy_crime_rates, Year >=2012 & Year <=2016) 
ggplot(crime_rates, aes(x = Year, y = rate)) + 
  geom_col(aes(fill = type)) 
```
###Shreeya Findings
Question: How do different crimes compare by rate? Does the change in definition for rape crimes affect the rates of these crimes from 2013 to 2016? How do the rate of violent crimes compare to nonviolent crime?

Findings: The amount of nonviolent crimes far outweigh the amount of violent crimes. In addition, the most common violent crime is aggravated assault. The revised definition of rape is a much higher rate than the legacy definition, which indicates that the revised definition is broader than the legacy definition. The average rate for the legacy definition is about 27 percent, which is much lower than the 37 percent for the revised definition. 


###James Burke Individual
```{r}

library(data.table)
library(tidyverse)

fires <- read_csv("/Users/jamesburke/Documents/Documents - Jamesâ€™s MacBook Pro/Data Science Labs/Lab8_littleRascals/cal-fire-incidents.csv")
fires <- as.tibble(fires)

fires <- mutate(fires, acres = `ACRES BURNED`/100)

ggplot(data = fires)+
  geom_line(mapping = aes(x = `YEAR`, y = `NUMBER OF FIRES`), color = "red")+
  geom_line(mapping = aes(x = `YEAR`, y = acres), color = "blue")+
  ylab("Number of Fires (Red), 100's Acres Burned (Blue)")

```
```{r}
fires <- mutate(fires, damage = `DOLLAR DAMAGE`/100000)
ggplot(data = fires)+
  geom_line(mapping = aes(x = `YEAR`, y = damage), color = "yellow")+
  ylab("100,000 Dollars in Damages")
```
###James Burke Findings
My question was how have the quantity and destruction of the fires in california changed between 1933 and 2016? I thought it was quite relevant due to the extreme fires that occured in california last year. I found that the acres burned peaked around the mid to late 1930s and the number of fires peaked in the 1980s, which would suggest that the fires in claifornia are not getting worse in general. However the graph of the cost of damages shows that in the 2000s and especially the 2010s the cost of damages has skyrocketed. This could be because the fires are closer to societies, which have become more dense as time has gone on.

###Matthew Copeland's Data
```{r}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(maps)
Olympics <- read.csv("summer.csv")
#total golds by country
golds <- Olympics %>% filter(Medal == "Gold") %>% count(Country) %>% arrange(desc(n)) 
medals <- Olympics %>% filter(Medal == "Gold") %>% count(Medal)
#top medal leaders of summer olympics
TML <- golds %>% filter(Country == c("USA","URS","GBR"))
Gpre <- golds %>% mutate((n/10486) * 100) %>% arrange(desc((n/10486) * 100))
A <- head(Gpre)
ggplot(data = A)+
  geom_col(aes(x = Country, y = (n/10486) * 100, fill = Country))+
  ylab("Precentage")+
  ggtitle("The Top Medal Leaders of the Summer Olympics\n and their Precentage of Winning a Gold Medal.")
```
##Matthews Question:
What is the probability of the top medal leaders winning a gold medal?
#Matthew's Findings:
I looked through a dataset that contained the olympic medal winners from 1896 to 2014 of the summer olympics. After tidying all the data I collected the probability of the top 6 gold medal leaders by dividing the total number of gold medals won by the countries gold medals in the past 118 years. I first sorted my data by getting rid of the rows where they weren't gold medals and had the columns country and medals. Then counted the duplicate countries so the list would drop to 100 countries and the gold medal count were the only columns. 

### Team Data
```{r}
library(data.table)
library(tidyverse)

tuition <- read_csv("us_avg_tuition.csv")

tuition <- tuition %>% select(State,'2004-05', '2014-15')
library(readr)
tuition <- tuition %>% mutate(four = parse_number(tuition$'2004-05')) %>%
  mutate(fourteen = parse_number(tuition$'2014-15'))
tuition <- tuition %>% select(State,four, fourteen) %>% arrange(desc(four)) %>% mutate(Difference = fourteen-four)
ggplot(data=tuition)+
  geom_point(mapping = aes(x=State, y=Difference , color = State))


```

### Team Conclusion
As you can see from the graph, the difference represents the tuition change between 2004/2005 and 2014/2015. The tuition change is positive for every single state except for one state. Therefore, tuition has increased overall over these ten years. Clearly, we can see that in some places the tuition increased by substantial amounts aswell. 
### Team Recommendation
Our recommendation is that the US government put measures in place to assist students more in tuition or rules that could maybe apply to state schools in order to mitigate college costs. Otherwise a college education would continue to get more and more expensive. 

###Team Summary
Kelsey McKenna
I uploaded the data in the form of csv instead of an excell file. I used the filter, select, and arrange functions in order to help answer my specific question. The data was pretty tidy already. The only problem was it had too many useless columns including a column of data with work status which wasn't useful. I also used header= T, na.strings, and stringsasFactors in order to tidy it further. I dealt with the team data. 

James Burke
I used read_csv() to read from the csv data file I downloaded about california fires. I did not have to tidy the data because every column had its own variable, every row had its own observation, every cell had its own value. I then made sure it was ina tibble by using as.tibble(). I plotted the number fires and the acres burned against the years on the same graph, so I can see how they fluctuated. I had to mutate the acres burned, by dividing it by 100 so it fit with similat y values, with number of fires. I would have added cost of damages to this graph but because its y values were so different it did not fit well.  

Shreeya 
First, I imported the data in excel file as a csv file. I tidied the data by fixing the headers, getting rid of the grouping marks and NAs and then reorganizing the format of the table into a tidy version with only year, rate and type variables. Finally, I filtered the table to only the years 2012-2016 to get a view of crime rates over a short amount of time and to be able to compare the revised and legacy definitions of rape. I did all this using select to get rid of junk and only focus on the important, column header is equal to true to fix the header and then gather to gather up the columns so only one variable is present in each column. 